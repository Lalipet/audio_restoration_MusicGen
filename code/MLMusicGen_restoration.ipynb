{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wd1sr59X9W8Y"
      },
      "source": [
        "##Setup\n",
        "\n",
        "This cell installs the necessary libraries to run the code, loads the pre-trained MusicGen small model (https://huggingface.co/facebook/musicgen-small), and initializes the generation pipeline. MusicGen generates music from text descriptions by converting text into audio tokens, predicting them using a transformer, and decoding them into 32kHz audio waveforms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8373O8qFM7L"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import random\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from scipy.signal import butter, sosfilt\n",
        "from transformers import AutoProcessor, MusicgenForConditionalGeneration\n",
        "from IPython.display import Audio, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egFs7Q0503X0"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# We use the 'musicgen-small' variant to balance audio quality\n",
        "# and computational efficiency, making the pipeline suitable\n",
        "# for Colab.\n",
        "\n",
        "model_id = \"facebook/musicgen-small\"\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "model = MusicgenForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "print(\"Ready on:\", device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6RTQcY6FTT7"
      },
      "source": [
        "## Music Generation\n",
        "\n",
        "We define a set of text prompts for different music genres and randomly select one for each genre. Then we use the MusicGen model to generate audio for each selected prompt. The generated audio is saved as WAV files.\n",
        "\n",
        "Prompts are intentionally descriptive: this choice ensures that differences in output quality are primarily due to the generation process and not prompt bias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvU1xaIq9y8w"
      },
      "outputs": [],
      "source": [
        "genre_prompts = {\n",
        "    \"pop\": [\n",
        "        \"Upbeat pop track with bright synths, steady four-on-the-floor drums, catchy melody throughout, consistent energy\",\n",
        "        \"Emotional pop song with strings, soft drums and piano, 80 BPM\",\n",
        "        \"Modern pop song with groovy bass and energetic drums\",\n",
        "        \"Dance-pop track with four-on-the-floor kick, pulsing bass and sparkling keys\",\n",
        "        \"Mid-tempo pop track with airy pads, dreamy atmosphere and soft percussion\"\n",
        "    ],\n",
        "    \"rock\": [\n",
        "        \"Energetic rock track with electric guitars, powerful bass, consistent energy throughout\",\n",
        "        \"Classic rock riff with electric guitar, steady bass and bluesy feel\",\n",
        "        \"Slow rock ballad with emotional lead guitar, soft drums, building intensity\",\n",
        "        \"Indie rock track with jangly guitars and tight drum groove, melodic bass, upbeat throughout\",\n",
        "        \"Heavy rock track with aggressive rhythm guitars, palm-muted riffs, thunderous drums, 130 BPM\"\n",
        "    ],\n",
        "    \"jazz\": [\n",
        "        \"Jazz song with saxophone lead melody, walking double bass, piano comping, brushed drums throughout\",\n",
        "        \"Smooth jazz track with electric piano and soft saxophone melody\",\n",
        "        \"Jazz fusion track with syncopated drums and fretless bass, energetic, 120 BPM\",\n",
        "        \"Big band jazz swing with brass section, walking bass, piano and saxophone\",\n",
        "        \"Atmospheric jazz ballad with trumpet and soft piano\"\n",
        "    ],\n",
        "    \"classical\": [\n",
        "        \"Romantic classical song for solo piano with expressive dynamics and flowing arpeggios\",\n",
        "        \"Piano playing a romantic, slow song, rich harmonies, cello melody, violin\",\n",
        "        \"Full symphony orchestra with sweeping strings and bold brass, dramatic atmosphere\",\n",
        "        \"Baroque-style piece with harpsichord and chamber ensemble, 110 BPM\",\n",
        "        \"Soft classical piece for piano and cello duet with gentle piano accompaniment, expressive, 60 BPM\"\n",
        "    ],\n",
        "    \"hiphop\": [\n",
        "        \"Hip-hop beat with dusty drums and vinyl crackle\",\n",
        "        \"Modern trap beat with deep bass and rapid hi-hats\",\n",
        "        \"Lo-fi hip-hop beat with warm piano samples and laid-back groove, chill atmosphere throughout\",\n",
        "        \"Aggressive hip-hop instrumental with heavy kicks and brass stabs\",\n",
        "        \"Chill hip-hop beat with soft Rhodes chords, subtle percussion, warm bass, relaxed vibe\"\n",
        "    ],\n",
        "    \"electronic\": [\n",
        "        \"Ambient electronic soundscape with evolving pads and soft pulses, 80 BPM\",\n",
        "        \"Melodic techno track with steady kick, hypnotic arpeggios, pulsing bass, building tension\",\n",
        "        \"Deep house groove with warm bass and airy chords\",\n",
        "        \"Dubstep-style track with bass and sharp snares\",\n",
        "        \"Future bass track with detuned synth chords and sidechain pumping\"\n",
        "    ],\n",
        "}\n",
        "\n",
        "selected_prompts = {}\n",
        "for genre, prompt_list in genre_prompts.items():\n",
        "    chosen = random.choice(prompt_list)\n",
        "    selected_prompts[genre] = chosen\n",
        "\n",
        "\n",
        "print(\"Chosen prompts:\")\n",
        "for genre, prompt in selected_prompts.items():\n",
        "    print(f\"- {genre}: {prompt}\")\n",
        "\n",
        "# MusicGen operates on discrete audio tokens; the number of\n",
        "# tokens directly controls output duration.\n",
        "\n",
        "duration_tokens = 700\n",
        "sr = model.config.audio_encoder.sampling_rate  # find the sampling rate\n",
        "print(f\"Sampling rate: {sr} Hz\")\n",
        "\n",
        "# Parameters are chosen to balance diversity and stability:\n",
        "# - temperature controls randomness\n",
        "# - top-k and top-p limit the sampling space\n",
        "# - guidance_scale is kept moderate to avoid over-conditioning\n",
        "\n",
        "for genre, prompt in selected_prompts.items():\n",
        "    print(f\"\\nGeneration for '{genre}'...\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    inputs = processor(\n",
        "        text=[prompt],\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        audio_values = model.generate(\n",
        "            **inputs,\n",
        "            do_sample=True,\n",
        "            guidance_scale=5,\n",
        "            max_new_tokens=duration_tokens,\n",
        "            temperature=0.7,\n",
        "            top_k=250,\n",
        "            top_p=0.9,\n",
        "        )\n",
        "\n",
        "    # Output shape: (batch, channels, samples)\n",
        "    audio = audio_values[0].detach().cpu().float().numpy()\n",
        "\n",
        "    display(Audio(audio, rate=sr))\n",
        "\n",
        "    filename = f\"raw_{genre}.wav\"\n",
        "    sf.write(filename, audio.T, sr)\n",
        "    print(f\"Saved as {filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_sLsyrJivPx"
      },
      "source": [
        "## Preprocessing: Normalization and High-Pass Filtering\n",
        "Through this function we first apply peak normalization to standardize signal amplitude across all samples.\n",
        "This step ensures that differences observed during restoration are not driven by loudness variations.  \n",
        "\n",
        "Then we apply a second-order Butterworth high-pass filter with a cutoff frequency of 30 Hz.\n",
        "This removes subsonic components that do not contribute to perceptual audio quality but may interfere with restoration models.\n",
        "This preprocessing step does not aim to improve audio quality directly, but to standardize the input signals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQB0m9fQ9iVL"
      },
      "outputs": [],
      "source": [
        "def preprocess_normalize_highpass(audio, sr, hp_cutoff=30.0):\n",
        "    audio = audio.astype(np.float32)\n",
        "\n",
        "    # stereo to mono\n",
        "    if audio.ndim == 2:\n",
        "        audio = np.mean(audio, axis=1)\n",
        "\n",
        "    # Peak normalization\n",
        "\n",
        "    # Normalize amplitude to a fixed peak level to ensure\n",
        "    # consistent loudness across samples\n",
        "\n",
        "    peak = np.max(np.abs(audio))\n",
        "    if peak > 0:\n",
        "        audio = 0.99 * (audio / peak)\n",
        "\n",
        "    # High-pass filtering\n",
        "    # Remove very low-frequency components (e.g., DC offset,\n",
        "    # that do not contribute to musical content\n",
        "    # but may interfere with restoration models.\n",
        "\n",
        "    sos_hp = butter(\n",
        "        N=2,\n",
        "        Wn=hp_cutoff,\n",
        "        btype=\"highpass\",\n",
        "        fs=sr,\n",
        "        output=\"sos\"\n",
        "    )\n",
        "\n",
        "    audio_hp = sosfilt(sos_hp, audio)\n",
        "    return audio_hp\n",
        "\n",
        "# Apply preprocessing to all generated samples\n",
        "genres = [\"pop\", \"rock\", \"jazz\", \"classical\", \"hiphop\", \"electronic\"]\n",
        "\n",
        "for genre in genres:\n",
        "    raw_filename = f\"raw_{genre}.wav\"\n",
        "    out_filename = f\"preproc_{genre}.wav\"\n",
        "\n",
        "    audio, sr = sf.read(raw_filename)\n",
        "    audio_pre = preprocess_normalize_highpass(audio, sr, hp_cutoff=30.0)\n",
        "\n",
        "    sf.write(out_filename, audio_pre, sr)\n",
        "    print(f\"Preprocess per {genre}: saved as {out_filename}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKBdpj1AepYb"
      },
      "source": [
        "###Preprocessing validation\n",
        "We verify the effectiveness of peak normalization and high pass filter by measuring peak amplitude and low frequency energy ratio below 30 hz.\n",
        "\n",
        "Peak amplitude is used to confirm that peak normalization successfully standardizes signal levels across samples.\n",
        "The low-frequency energy ratio serves as an objective indicator of subsonic content, which should be reduced by the high-pass filter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdknm9NKd7MQ"
      },
      "outputs": [],
      "source": [
        "def peak_amplitude(x):\n",
        "\n",
        "    '''\n",
        "    Compute peak (maximum absolute) amplitude of an audio signal.\n",
        "    Used to verify the effectiveness of peak normalization\n",
        "\n",
        "    '''\n",
        "\n",
        "    return np.max(np.abs(x))\n",
        "\n",
        "\n",
        "def lf_energy_ratio(x, sr, lf_max_hz=30.0):\n",
        "    '''\n",
        "    Compute the ratio of low-frequency energy below a given cutoff.\n",
        "    This metric quantifies the proportion of signal energy contained\n",
        "    in subsonic frequencies which should have\n",
        "    been reduced by high-pass filtering\n",
        "\n",
        "    '''\n",
        "\n",
        "    X = np.fft.rfft(x)\n",
        "    freqs = np.fft.rfftfreq(len(x), d=1.0 / sr)\n",
        "    power = np.abs(X) ** 2\n",
        "\n",
        "    lf_energy = np.sum(power[freqs <= lf_max_hz])\n",
        "    total_energy = np.sum(power) + 1e-12\n",
        "\n",
        "    return lf_energy / total_energy\n",
        "\n",
        "def load_mono(path):\n",
        "\n",
        "    audio, sr = sf.read(path, dtype=\"float32\")\n",
        "    if audio.ndim == 2:\n",
        "        audio = np.mean(audio, axis=1)\n",
        "    return audio, sr\n",
        "\n",
        "# We quantitatively evaluate the effect of preprocessing by\n",
        "# comparing raw and preprocessed signals\n",
        "\n",
        "genres = [\"pop\", \"rock\", \"jazz\", \"classical\", \"hiphop\", \"electronic\"]\n",
        "\n",
        "rows = []\n",
        "\n",
        "for genre in genres:\n",
        "    raw_path = f\"raw_{genre}.wav\"\n",
        "    pre_path = f\"preproc_{genre}.wav\"\n",
        "\n",
        "    raw_audio, sr = load_mono(raw_path)\n",
        "    pre_audio, _ = load_mono(pre_path)\n",
        "\n",
        "    rows.append({\n",
        "        \"genre\": genre,\n",
        "        \"stage\": \"raw\",\n",
        "        \"peak\": peak_amplitude(raw_audio),\n",
        "        \"lf_ratio_<30Hz\": lf_energy_ratio(raw_audio, sr)\n",
        "    })\n",
        "\n",
        "    rows.append({\n",
        "        \"genre\": genre,\n",
        "        \"stage\": \"preproc\",\n",
        "        \"peak\": peak_amplitude(pre_audio),\n",
        "        \"lf_ratio_<30Hz\": lf_energy_ratio(pre_audio, sr)\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "df = df.sort_values(by=[\"genre\", \"stage\"])\n",
        "\n",
        "print(\"Preprocessing validation:\")\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM3wVlc3ucDi"
      },
      "source": [
        "## Denoising: noise reduction with Noisereduce library\n",
        "We reduce background noise from each preprocessed audio clip using the noisereduce library. The function estimates a noise profile, and applies non-stationary noise reduction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cGIFppUHPHs"
      },
      "outputs": [],
      "source": [
        "!pip install -q noisereduce\n",
        "import noisereduce as nr\n",
        "\n",
        "def denoise_noisereduce_mono(audio, sr):\n",
        "    audio = audio.astype(np.float32)\n",
        "\n",
        "    \"\"\"\n",
        "    Apply non-stationary noise reduction to a mono audio signal.\n",
        "    Although MusicGen does not generate environmental noise, its outputs\n",
        "    often contain low-level artifacts and noise-like components.\n",
        "    This function treats such artifacts as non-stationary noise and\n",
        "    applies spectral attenuation.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if audio.ndim == 2:\n",
        "        audio = np.mean(audio, axis=1)\n",
        "\n",
        "    y_denoised = nr.reduce_noise(\n",
        "        y=audio,\n",
        "        y_noise=None,       # noise profile estimated automatically\n",
        "        sr=sr,\n",
        "        prop_decrease=0.8,  # noise attenuation percentage\n",
        "        stationary=False    # assuming non-stationary noise\n",
        "    )\n",
        "\n",
        "    return y_denoised\n",
        "\n",
        "    # Apply denoising to preprocessed audio\n",
        "\n",
        "for genre in genres:\n",
        "\n",
        "    pre_file = f\"preproc_{genre}.wav\"\n",
        "    out_file = f\"denoised_{genre}.wav\"\n",
        "\n",
        "    print(f\"\\nGenre: {genre}\")\n",
        "\n",
        "    audio_pre, sr = sf.read(pre_file)\n",
        "    print(\"Preprocessed:\")\n",
        "    display(Audio(audio_pre, rate=sr))\n",
        "\n",
        "    audio_denoised = denoise_noisereduce_mono(audio_pre, sr)\n",
        "    print(\"Denoised:\")\n",
        "    display(Audio(audio_denoised, rate=sr))\n",
        "\n",
        "    # Save denoised file\n",
        "    sf.write(out_file, audio_denoised, sr)\n",
        "    print(f\"Saved as {out_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VD2CEpss4MaS"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "def plot_spectrogram(y, sr, title, fmax=None):\n",
        "\n",
        "    S = librosa.stft(y, n_fft=2048, hop_length=512)\n",
        "    S_db = librosa.amplitude_to_db(np.abs(S), ref=np.max)\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.specshow(\n",
        "        S_db,\n",
        "        sr=sr,\n",
        "        hop_length=512,\n",
        "        x_axis=\"time\",\n",
        "        y_axis=\"hz\"\n",
        "    )\n",
        "\n",
        "    if fmax is not None:\n",
        "        plt.ylim(0, fmax)\n",
        "\n",
        "    plt.colorbar(format=\"%+2.0f dB\")\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "genres = [\"pop\", \"rock\", \"jazz\", \"classical\", \"hiphop\", \"electronic\"]\n",
        "\n",
        "for genre in genres:\n",
        "    print(f\"\\n Genre: {genre.upper()}\")\n",
        "\n",
        "    y_pre, sr = sf.read(f\"preproc_{genre}.wav\")\n",
        "    y_den, _ = sf.read(f\"denoised_{genre}.wav\")\n",
        "\n",
        "    plot_spectrogram(\n",
        "        y_pre, sr,\n",
        "        title=f\"{genre} – Preprocessed\",\n",
        "        fmax=16000\n",
        "    )\n",
        "\n",
        "    plot_spectrogram(\n",
        "        y_den, sr,\n",
        "        title=f\"{genre} – Denoised\",\n",
        "        fmax=16000\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUbuCQlJ57w-"
      },
      "source": [
        "## Bandwidth Extension with HiFi‑GAN BWE (from 24 kHz to 48 kHz)\n",
        "\n",
        "We use a bandwidth‑extension model (HiFi‑GAN BWE, https://github.com/brentspell/hifi-gan-bwe) (which is a third‑party implementation and not the official HiFi‑GAN release) to enhance the bandwidth of the denoised\n",
        "audio clips.  \n",
        "Each cleaned audio file (assumed to be mono or converted to mono) is downsampled to the 24 kHz input rate expected by the model, and feed it to the BWE model to generate a 48 kHz version with reconstructed high‑frequency content.\n",
        "\n",
        "If the BWE model fails for any reason, a standard resampling fallback  is applied to produce a 48 kHz output.  \n",
        "\n",
        "- ⚠️**WARNING!**⚠️ we note that the adopted BWE model is primarily optimized for speech signals rather than music.\n",
        "Its application in this context is therefore exploratory and aims to assess whether generative bandwidth extension can improve the perceived fidelity of music generated by text-to-audio models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8iPYTb3Nkz-"
      },
      "outputs": [],
      "source": [
        "!pip install -q hifi-gan-bwe\n",
        "import warnings\n",
        "import os\n",
        "from hifi_gan_bwe import BandwidthExtender\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"starting BWE...\")\n",
        "\n",
        "# Setup\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "if 'bwe_model' not in locals():\n",
        "    bwe_model = BandwidthExtender.from_pretrained(\"hifi-gan-bwe-13-59f00ca-vctk-24kHz-48kHz\").to(device)\n",
        "    bwe_model.eval()\n",
        "\n",
        "target_sr_out = 48000\n",
        "fs_in = 24000\n",
        "\n",
        "# Apply bandwidth extension to denoised audio\n",
        "\n",
        "\n",
        "for genre in genres:\n",
        "    in_path = f\"denoised_{genre}.wav\"\n",
        "    out_path = f\"bwe_{genre}_48k.wav\"\n",
        "\n",
        "    if not os.path.exists(in_path):\n",
        "        print(f\"Skipping {genre}: file not found.\")\n",
        "        continue\n",
        "\n",
        "    y, sr = sf.read(in_path, dtype=\"float32\")\n",
        "\n",
        "    # Ensure mono input\n",
        "\n",
        "    if y.ndim == 2:\n",
        "        y = np.mean(y, axis=1)\n",
        "\n",
        "    x = torch.from_numpy(y).float().unsqueeze(0).unsqueeze(0).to(device)\n",
        "\n",
        "    # Resample to model input rate if needed\n",
        "\n",
        "    if sr != fs_in:\n",
        "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=fs_in).to(device)\n",
        "        x = resampler(x)\n",
        "        current_sr = fs_in\n",
        "    else:\n",
        "        current_sr = sr\n",
        "\n",
        "    print(f\"Processing {genre}...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "      try:\n",
        "          y_48k_t = bwe_model(x, current_sr)\n",
        "          y_48k = y_48k_t.squeeze().cpu().numpy()\n",
        "          print(f\"{genre}: BWE Success\")\n",
        "\n",
        "      except Exception as e:\n",
        "          print(f\"{genre}: BWE Failed. Error: {e}\")\n",
        "          print(\" -> Fallback: standard resampling\")\n",
        "\n",
        "          y_48k_t = torchaudio.functional.resample(\n",
        "              x, orig_freq=current_sr, new_freq=target_sr_out\n",
        "          )\n",
        "          y_48k = y_48k_t.squeeze().cpu().numpy()\n",
        "\n",
        "\n",
        "    sf.write(out_path, y_48k, target_sr_out)\n",
        "\n",
        "    print(f\"Preview {genre}:\")\n",
        "    display(Audio(y_48k, rate=target_sr_out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rURE-S2Aqgix"
      },
      "outputs": [],
      "source": [
        "genres = [\"pop\", \"rock\", \"jazz\", \"classical\", \"hiphop\", \"electronic\"]\n",
        "\n",
        "def plot_spectrogram(y, sr, title, fmax=None):\n",
        "    S = librosa.stft(y, n_fft=2048, hop_length=512)\n",
        "    S_db = librosa.amplitude_to_db(np.abs(S), ref=np.max)\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.specshow(\n",
        "        S_db,\n",
        "        sr=sr,\n",
        "        hop_length=512,\n",
        "        x_axis=\"time\",\n",
        "        y_axis=\"hz\"\n",
        "    )\n",
        "    if fmax is not None:\n",
        "        plt.ylim(0, fmax)\n",
        "    plt.colorbar(format=\"%+2.0f dB\")\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "for genre in genres:\n",
        "    print(f\"\\n=== GENRE: {genre.upper()} ===\")\n",
        "\n",
        "    den_path = f\"denoised_{genre}.wav\"\n",
        "    bwe_path = f\"bwe_{genre}_48k.wav\"\n",
        "\n",
        "    y_den, sr_den = sf.read(den_path)\n",
        "    y_bwe, sr_bwe = sf.read(bwe_path)\n",
        "\n",
        "    # stereo → mono\n",
        "    if y_den.ndim == 2:\n",
        "        y_den = y_den.mean(axis=1)\n",
        "    if y_bwe.ndim == 2:\n",
        "        y_bwe = y_bwe.mean(axis=1)\n",
        "\n",
        "    plot_spectrogram(\n",
        "        y_den, sr_den,\n",
        "        title=f\"{genre} – Denoised\",\n",
        "        fmax=24000\n",
        "    )\n",
        "\n",
        "    plot_spectrogram(\n",
        "        y_bwe, sr_bwe,\n",
        "        title=f\"{genre} – BWE Output (48 kHz)\",\n",
        "        fmax=24000\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}